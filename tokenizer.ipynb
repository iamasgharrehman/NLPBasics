{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a corpus and apply sentence tokenizer on top of that corpus. Also install the necessary libraries to perform the described task and write comment for each step performed\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "id": "BijFRB6NaXWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the punkt_tab resource\n",
        "nltk.download('punkt_tab') # This line downloads the necessary data\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sample corpus (replace with your actual corpus)\n",
        "corpus = \"\"\"This is the first sentence. This is the second sentence. This is the third sentence! This is the fourth sentence?\"\"\"\n",
        "\n",
        "# Apply sentence tokenization\n",
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "# Print the tokenized sentences\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zYDbYKYaXaM",
        "outputId": "235203cc-c106-4c37-ca11-b0de378576ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is the first sentence.',\n",
              " 'This is the second sentence.',\n",
              " 'This is the third sentence!',\n",
              " 'This is the fourth sentence?']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIfQrujaa1FT",
        "outputId": "792665df-d9a0-4965-8bb9-0a5709b04e2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "    print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hitjwZ6Za45f",
        "outputId": "c95136f5-3beb-4ea1-e09c-5c3fd4518896"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "This is the second sentence.\n",
            "This is the third sentence!\n",
            "This is the fourth sentence?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenizer -> using word tokenizer\n",
        "## Parhgraph(corpus to words)\n",
        "## senctence (document to words)\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)\n",
        "print(type(words))\n",
        "print(words)\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aRUTc_Pta9aE",
        "outputId": "ec650b15-6a52-43ab-9f87-d80aa6203e55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['This', 'is', 'the', 'first', 'sentence', '.', 'This', 'is', 'the', 'second', 'sentence', '.', 'This', 'is', 'the', 'third', 'sentence', '!', 'This', 'is', 'the', 'fourth', 'sentence', '?']\n",
            "This\n",
            "is\n",
            "the\n",
            "first\n",
            "sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "second\n",
            "sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "third\n",
            "sentence\n",
            "!\n",
            "This\n",
            "is\n",
            "the\n",
            "fourth\n",
            "sentence\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "    print(word_tokenize(document))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ugLAgga9oW",
        "outputId": "c0c01885-4c76-4d18-b2c3-b4e26edc7e66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.']\n",
            "['This', 'is', 'the', 'second', 'sentence', '.']\n",
            "['This', 'is', 'the', 'third', 'sentence', '!']\n",
            "['This', 'is', 'the', 'fourth', 'sentence', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # punctuation also treated as seperate token\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DXjtUubjb9L5",
        "outputId": "0703bb73-7865-4771-d9fa-770080e9242b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'sentence',\n",
              " '!',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'fourth',\n",
              " 'sentence',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =\"\"\"\n",
        "Hellow welcome to this amazing NLP Tutorials.\n",
        "Please vist the entire github repo and follow the instrucions provided in README! Thank you\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kjUeJSRQcUsT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "MqxrJy27cUva"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TreebankWordTokenizer().tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMtoCpa8cUyd",
        "outputId": "b0e9de74-1544-4ed4-80a3-dad9f07ceb35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hellow',\n",
              " 'welcome',\n",
              " 'to',\n",
              " 'this',\n",
              " 'amazing',\n",
              " 'NLP',\n",
              " 'Tutorials.',\n",
              " 'Please',\n",
              " 'vist',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'github',\n",
              " 'repo',\n",
              " 'and',\n",
              " 'follow',\n",
              " 'the',\n",
              " 'instrucions',\n",
              " 'provided',\n",
              " 'in',\n",
              " 'README',\n",
              " '!',\n",
              " 'Thank',\n",
              " 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRU-3wqTcU1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}